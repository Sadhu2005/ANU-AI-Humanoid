markdown
# ANU 6.0 - Artificial Neural Universe ğŸ¤–

<p align="center">
<img src="https://via.placeholder.com/600x300/134e4a/f5f5f4?text=ANU+6.0" alt="ANU 6.0 Project Banner">
</p>

<p align="center">
<strong>An AI-powered humanoid robot designed to be a personalized mentor and an intelligent friend for rural students in India.</strong>
<br />
<br />
<img src="https://img.shields.io/badge/Status-In%20Development-blue" alt="Status">
<img src="https://img.shields.io/badge/Python-3.9%2B-blueviolet" alt="Python Version">
<img src="https://img.shields.io/badge/Platform-Raspberry%20Pi%205-orange" alt="Platform">
<img src="https://img.shields.io/badge/License-MIT-green" alt="License">
</p>

## ğŸ¯ Our Mission

To bridge the educational and digital divide for students in rural Karnataka by providing an accessible, engaging, and distraction-free AI learning companion. ANU 6.0 aims to build English fluency, foster confidence, and become a gateway to technology and information for students and their households.

*This project was initiated by the EvoBot Crew at the Coorg Institute of Technology, Ponnampet.*

## ğŸ§ The Problem

Rural students face significant hurdles in their educational journey:

- **Lack of Conversational Practice**: No environment to practice spoken English, limiting fluency.
- **The Distraction Dilemma**: Mobile phones, while a resource, are a primary source of distraction.
- **Resource Scarcity**: Limited access to personalized, high-quality teaching aids.
- **The Foundational Gap**: A weak foundation in English creates barriers to higher education and careers in technology.

## âœ¨ Our Solution: ANU 6.0

ANU 6.0 is a 17-DOF humanoid robot that acts as a physical, AI-powered mentor. It provides a focused, interactive, and bilingual (Kannada & English) learning experience, adapting to each student's unique pace and needs. It's a friend to practice with, a teacher to learn from, and a tool that makes advanced technology accessible and fun.

### Key Features

- ğŸ—£ï¸ **Bilingual Conversation**: Fluent in both Kannada and English for natural interaction.
- ğŸ“ **Personalized Learning Paths**: Assesses student levels and creates custom lesson plans.
- ğŸ¤– **Physical & Engaging**: Performs gestures like waving and hugging to create a strong, friendly bond.
- ğŸŒ **Hybrid AI Brain**: Functions offline with on-device models and syncs with powerful cloud APIs when online.
- ğŸ‘€ **Computer Vision**: Uses OpenCV for face recognition and environmental awareness.
- ğŸ“µ **Distraction-Free**: A dedicated device for learning, free from the distractions of mobile phones.

## ğŸš€ Project Architecture

The system follows a modular, three-stage flow:

1. **Perception (Input)**: The robot gathers data using its webcam (visuals), microphone (voice commands), and environmental sensors (proximity, presence).
2. **Processing (The Brain)**: The Raspberry Pi 5 processes this data. It transcribes speech, analyzes images, and uses its hybrid AI core (local models + cloud APIs) to make decisions.
3. **Actuation (Output)**: The brain sends commands to the PCA9685 servo driver to perform physical actions and to the speaker to generate verbal responses.

## ğŸ“ Repository Structure
anu-6.0/
â”‚
â”œâ”€â”€ ğŸ¤– robot_brain/ # All code that runs on the Raspberry Pi
â”‚ â”œâ”€â”€ main.py # Main application entry point
â”‚ â”œâ”€â”€ requirements.txt # Python dependencies for the robot
â”‚ â”œâ”€â”€ .env.example # Example environment variables
â”‚ â”œâ”€â”€ config/ # Configuration files
â”‚ â”‚ â”œâ”€â”€ settings.py # Main configuration settings
â”‚ â”‚ â””â”€â”€ servo_config.json # Servo calibration data
â”‚ â”‚
â”‚ â”œâ”€â”€ core/ # Core system modules
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ state_manager.py # Manages the robot's current state
â”‚ â”‚ â””â”€â”€ connectivity_manager.py # Handles online/offline status
â”‚ â”‚
â”‚ â”œâ”€â”€ modules/ # Individual functionality modules
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ speech_processing/ # Handles STT and TTS
â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”‚ â”œâ”€â”€ speech_recognizer.py
â”‚ â”‚ â”‚ â”œâ”€â”€ text_to_speech.py
â”‚ â”‚ â”‚ â””â”€â”€ voice_processor.py
â”‚ â”‚ â”‚
â”‚ â”‚ â”œâ”€â”€ ai_core/ # The AI decision-making brain
â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”‚ â”œâ”€â”€ gemini_api.py # Gemini API integration
â”‚ â”‚ â”‚ â”œâ”€â”€ conversation_manager.py
â”‚ â”‚ â”‚ â””â”€â”€ response_generator.py
â”‚ â”‚ â”‚
â”‚ â”‚ â”œâ”€â”€ vision/ # OpenCV and camera functionalities
â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”‚ â”œâ”€â”€ camera_controller.py
â”‚ â”‚ â”‚ â””â”€â”€ face_recognition.py
â”‚ â”‚ â”‚
â”‚ â”‚ â””â”€â”€ motion/ # Controls physical movements
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ servo_controller.py # Interface with PCA9685 driver
â”‚ â”‚ â”œâ”€â”€ motion_planner.py
â”‚ â”‚ â””â”€â”€ gestures_library.py # Predefined gestures and actions
â”‚ â”‚
â”‚ â”œâ”€â”€ utils/ # Utility functions and helpers
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ logger.py # Logging configuration
â”‚ â”‚ â”œâ”€â”€ helpers.py # Common helper functions
â”‚ â”‚ â””â”€â”€ audio_utils.py # Audio processing utilities
â”‚ â”‚
â”‚ â””â”€â”€ tests/ # Unit and integration tests
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ test_speech.py
â”‚ â”œâ”€â”€ test_vision.py
â”‚ â””â”€â”€ test_motion.py
â”‚
â”œâ”€â”€ ğŸŒ server/ # (Optional) Backend server code
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â””â”€â”€ api/ # API endpoints
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ routes.py
â”‚
â”œâ”€â”€ ğŸ› ï¸ hardware/ # Hardware schematics and documentation
â”‚ â”œâ”€â”€ ANU_6.0_BOM.csv # Bill of Materials
â”‚ â”œâ”€â”€ wiring_diagram.png
â”‚ â”œâ”€â”€ assembly_guide.md
â”‚ â””â”€â”€ 3d_models/ # STL files for 3D printed parts
â”‚ â”œâ”€â”€ head.stl
â”‚ â”œâ”€â”€ arm.stl
â”‚ â””â”€â”€ torso.stl
â”‚
â”œâ”€â”€ ğŸ“š docs/ # Project documentation
â”‚ â”œâ”€â”€ architecture.md
â”‚ â”œâ”€â”€ setup_guide.md
â”‚ â”œâ”€â”€ api_reference.md
â”‚ â””â”€â”€ contribution_guide.md
â”‚
â”œâ”€â”€ ğŸ“¦ datasets/ # Training data and models
â”‚ â”œâ”€â”€ voice_samples/ # Voice command samples
â”‚ â”œâ”€â”€ face_data/ # Face recognition training data
â”‚ â””â”€â”€ models/ # Pre-trained models
â”‚ â”œâ”€â”€ vosk-model/ # Offline speech recognition model
â”‚ â””â”€â”€ face_recognition_model/
â”‚
â”œâ”€â”€ scripts/ # Deployment and maintenance scripts
â”‚ â”œâ”€â”€ install_dependencies.sh
â”‚ â”œâ”€â”€ setup_raspberrypi.sh
â”‚ â””â”€â”€ update_robot.sh
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ CODE_OF_CONDUCT.md
â””â”€â”€ README.md

text

## âš™ï¸ Getting Started

### Prerequisites

- Raspberry Pi 5 (8GB RAM recommended) with Raspberry Pi OS (64-bit)
- Python 3.9 or newer
- All hardware components (servos, drivers, camera, microphone, etc.)
- Internet connection (for initial setup)

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-username/anu-6.0.git
   cd anu-6.0/robot_brain
Create a virtual environment:

bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
Install dependencies:

bash
pip install -r requirements.txt
Set up environment variables:

bash
cp .env.example .env
# Edit .env with your API keys and configuration
Hardware setup:

Follow the hardware assembly guide in /hardware/assembly_guide.md

Connect all servos to the PCA9685 controller

Connect camera and microphone to Raspberry Pi

Configuration
Edit the .env file with your specific configuration:

env
# API Keys
GEMINI_API_KEY=your_gemini_api_key_here

# Speech Recognition
SPEECH_RECOGNITION_ENGINE=google  # google or vosk
VOSK_MODEL_PATH=./datasets/models/vosk-model

# Text-to-Speech
TTS_ENGINE=pyttsx3  # pyttsx3, gtts, or espeak

# Camera Settings
CAMERA_RESOLUTION=640x480
CAMERA_FPS=30

# Servo Configuration
SERVO_FREQUENCY=50
SERVO_MIN_PULSE=500
SERVO_MAX_PULSE=2500
Running the Robot
bash
# Activate virtual environment
source .venv/bin/activate

# Run the main application
python main.py

# Or run with specific modules
python -m modules.speech_processing.voice_processor
ğŸ§ª Testing
Run the test suite to verify all components are working:

bash
# Run all tests
python -m pytest tests/

# Run specific test modules
python -m pytest tests/test_speech.py -v
python -m pytest tests/test_vision.py -v
python -m pytest tests/test_motion.py -v
ğŸ¤ Contributing
We welcome contributions! Please read our Contribution Guide for details on our code of conduct and the process for submitting pull requests.

Development Setup
Fork the repository

Create a feature branch: git checkout -b feature/amazing-feature

Commit your changes: git commit -m 'Add amazing feature'

Push to the branch: git push origin feature/amazing-feature

Open a pull request

ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ™ Acknowledgments
Coorg Institute of Technology, Ponnampet

EvoBot Crew team members

Open source communities for various libraries and tools

Raspberry Pi Foundation for hardware support

ğŸ“ Contact
For questions or support, please contact:

Project Lead: [Your Name] - [your.email@example.com]

Development Team: [team.email@example.com]

ğŸ”— Useful Links
Project Wiki

Issue Tracker

Release Notes

<p align="center"> Made with â¤ï¸ by the EvoBot Crew | Coorg Institute of Technology </p> ```